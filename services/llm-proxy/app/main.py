import os, httpx, fastapi, time, uuid, random
import structlog
import logging
from datetime import datetime
from pydantic import BaseModel
from pythonjsonlogger import jsonlogger

# Configure structured logging
logging.basicConfig(level=logging.INFO)
logHandler = logging.StreamHandler()
formatter = jsonlogger.JsonFormatter(
    fmt="%(asctime)s %(name)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logHandler.setFormatter(formatter)
logger = logging.getLogger("llm-proxy")
logger.addHandler(logHandler)
logger.setLevel(logging.INFO)

# Structured logger for metrics
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

metrics_logger = structlog.get_logger("llm-metrics")

AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_KEY      = os.getenv("AOAI_KEY")

if not AOAI_ENDPOINT or not AOAI_KEY:
    raise RuntimeError(
        "Environment variables AOAI_ENDPOINT and AOAI_KEY must be set "
        "for llmâ€‘proxy to start."
    )

app = fastapi.FastAPI()

class ChatReq(BaseModel):
    prompt: str
    model: str = "gpt-4"
    max_tokens: int = 150
    temperature: float = 0.7

def calculate_tokens(text: str) -> int:
    """Simple token estimation - in production use tiktoken or similar"""
    return len(text.split()) * 1.3  # Rough approximation

def calculate_energy_consumption(tokens_used: int, model: str) -> float:
    """Estimate energy consumption based on tokens and model"""
    # Energy estimates in kWh per 1000 tokens (hypothetical values)
    energy_rates = {
        "gpt-4": 0.005,
        "gpt-3.5-turbo": 0.002,
        "gpt-4-turbo": 0.004,
        "default": 0.003
    }
    rate = energy_rates.get(model, energy_rates["default"])
    return (tokens_used / 1000) * rate

def calculate_cost(input_tokens: int, output_tokens: int, model: str) -> float:
    """Calculate cost based on OpenAI pricing"""
    pricing = {
        "gpt-4": {"input": 0.03, "output": 0.06},  # per 1k tokens
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
        "default": {"input": 0.01, "output": 0.02}
    }
    rates = pricing.get(model, pricing["default"])
    return (input_tokens / 1000 * rates["input"]) + (output_tokens / 1000 * rates["output"])

@app.post("/chat")
async def chat(req: ChatReq):
    request_id = str(uuid.uuid4())
    start_time = time.time()
    
    # Log request start
    logger.info("LLM request started", extra={
        "request_id": request_id,
        "model": req.model,
        "max_tokens": req.max_tokens,
        "temperature": req.temperature,
        "prompt_length": len(req.prompt)
    })
    
    try:
        headers = {"api-key": AOAI_KEY, "Content-Type": "application/json"}
        endpoint = AOAI_ENDPOINT
        
        # Calculate input tokens
        input_tokens = calculate_tokens(req.prompt)
        
        # Simulate LLM call (replace with actual call)
        #async with httpx.AsyncClient() as cx:
        #    r = await cx.post(AOAI_ENDPOINT, headers=headers, json=req.dict())
        #    response_data = r.json()
        
        # Mock response for demo
        response_text = f"This is a mock response to: {req.prompt[:50]}... [Generated by {req.model}]"
        response_data = {"choices": [{"message": {"content": response_text}}]}
        
        # Calculate metrics
        output_tokens = calculate_tokens(response_text)
        total_tokens = input_tokens + output_tokens
        processing_time = time.time() - start_time
        energy_consumption = calculate_energy_consumption(total_tokens, req.model)
        cost = calculate_cost(input_tokens, output_tokens, req.model)
        
        # Log comprehensive metrics
        metrics_logger.info("llm_request_completed", 
            request_id=request_id,
            model=req.model,
            input_tokens=int(input_tokens),
            output_tokens=int(output_tokens),
            total_tokens=int(total_tokens),
            processing_time_seconds=round(processing_time, 3),
            energy_consumption_kwh=round(energy_consumption, 6),
            cost_usd=round(cost, 4),
            temperature=req.temperature,
            max_tokens=req.max_tokens,
            prompt_chars=len(req.prompt),
            response_chars=len(response_text),
            timestamp=datetime.utcnow().isoformat(),
            status="success"
        )
        
        # Log business metrics
        logger.info("LLM request completed successfully", extra={
            "request_id": request_id,
            "processing_time": round(processing_time, 3),
            "total_tokens": int(total_tokens),
            "cost": round(cost, 4),
            "energy_kwh": round(energy_consumption, 6)
        })
        
        return {
            "response": response_data,
            "metadata": {
                "request_id": request_id,
                "tokens_used": int(total_tokens),
                "processing_time": round(processing_time, 3),
                "energy_consumption_kwh": round(energy_consumption, 6),
                "cost_usd": round(cost, 4)
            }
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        
        # Log error
        logger.error("LLM request failed", extra={
            "request_id": request_id,
            "error": str(e),
            "processing_time": round(processing_time, 3)
        })
        
        metrics_logger.error("llm_request_failed",
            request_id=request_id,
            model=req.model,
            error=str(e),
            processing_time_seconds=round(processing_time, 3),
            timestamp=datetime.utcnow().isoformat(),
            status="error"
        )
        
        raise fastapi.HTTPException(status_code=500, detail=f"LLM request failed: {str(e)}")


@app.get("/healthz")
async def health() -> dict[str, str]:
    return {"status": "ok"}