name: Deploy MLOps Platform
on: 
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AZURE_RESOURCE_GROUP: mlops-dev-rg
  AKS_CLUSTER_NAME: aks-mlops-dev
  IMAGE_GUARDRAIL: ghcr.io/${{ github.repository_owner }}/guardrail:${{ github.sha }}
  IMAGE_PROXY: ghcr.io/${{ github.repository_owner }}/llm-proxy:${{ github.sha }}

jobs:
  infrastructure:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    outputs:
      azure_openai_endpoint: ${{ steps.terraform-outputs.outputs.azure_openai_endpoint }}
      azure_openai_api_key: ${{ steps.terraform-outputs.outputs.azure_openai_api_key }}
      gpt4o_deployment_name: ${{ steps.terraform-outputs.outputs.gpt4o_deployment_name }}
      frontdoor_endpoint_hostname: ${{ steps.terraform-outputs.outputs.frontdoor_endpoint_hostname }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0
      
      - name: Set Terraform Azure Provider Environment Variables
        run: |
          # Extract values from AZURE_CREDENTIALS secret
          CLIENT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientId')
          CLIENT_SECRET=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientSecret')
          TENANT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.tenantId')
          SUBSCRIPTION_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.subscriptionId')
          
          # Set environment variables for Terraform Azure Provider
          echo "ARM_CLIENT_ID=$CLIENT_ID" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=$CLIENT_SECRET" >> $GITHUB_ENV
          echo "ARM_TENANT_ID=$TENANT_ID" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=$SUBSCRIPTION_ID" >> $GITHUB_ENV
          echo "ARM_USE_CLI=false" >> $GITHUB_ENV
      
      - name: Terraform Init
        working-directory: infra/envs/azure/dev
        run: terraform init
      
      - name: Terraform Plan
        working-directory: infra/envs/azure/dev
        run: terraform plan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        working-directory: infra/envs/azure/dev
        run: |
          # Apply infrastructure changes
          terraform apply -auto-approve

      - name: Get Terraform outputs
        id: terraform-outputs
        working-directory: infra/envs/azure/dev
        run: |
          echo "azure_openai_endpoint=$(terraform output -raw azure_openai_endpoint)" >> $GITHUB_OUTPUT
          echo "azure_openai_api_key=$(terraform output -raw azure_openai_api_key)" >> $GITHUB_OUTPUT
          echo "gpt4o_deployment_name=$(terraform output -raw gpt4o_deployment_name)" >> $GITHUB_OUTPUT
          echo "frontdoor_endpoint_hostname=$(terraform output -raw frontdoor_endpoint_hostname)" >> $GITHUB_OUTPUT

  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - name: Log in to GitHub Container Registry
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build & push Guardrail
        run: |
          docker build -t $IMAGE_GUARDRAIL services/guardrail
          docker push $IMAGE_GUARDRAIL

      - name: Build & push LLM-proxy
        run: |
          docker build -t $IMAGE_PROXY services/llm-proxy
          docker push $IMAGE_PROXY

  deploy:
    needs: [infrastructure, build]
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Setup kubectl with Azure AD
        run: |
          # Use admin credentials for simplicity - bypasses RBAC complexity
          # The service principal has Azure Kubernetes Service Cluster Admin Role
          echo "Getting admin credentials for AKS cluster..."
          az aks get-credentials \
            --resource-group ${{ env.AZURE_RESOURCE_GROUP }} \
            --name ${{ env.AKS_CLUSTER_NAME }} \
            --admin \
            --overwrite-existing
          
          # Test cluster access
          kubectl auth can-i get nodes && echo "âœ… Cluster access confirmed" || echo "âŒ Cluster access failed"

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Add Helm repositories
        run: |
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add kong https://charts.konghq.com
          helm repo update

      - name: Smart Cleanup and Preparation
        run: |
          echo "ðŸ§¹ Smart cleanup of potential conflicts..."
          
          # Check if llm namespace exists
          if kubectl get namespace llm &>/dev/null; then
            echo "ðŸ“‹ LLM namespace exists - performing targeted cleanup..."
            
            # Uninstall Helm releases first (cleaner than deleting namespace)
            echo "Cleaning up Helm releases in llm namespace..."
            helm uninstall kong -n llm --ignore-not-found || echo "Kong not found"
            helm uninstall guardrail -n llm --ignore-not-found || echo "Guardrail not found"
            helm uninstall llm-proxy -n llm --ignore-not-found || echo "LLM-proxy not found"
            helm uninstall gateway -n llm --ignore-not-found || echo "Gateway not found"
            
            # Clean up remaining resources that might conflict
            echo "Cleaning up potentially conflicting resources..."
            kubectl delete serviceaccount --all -n llm --ignore-not-found || echo "ServiceAccounts cleanup completed"
            kubectl delete secret ghcr-pull -n llm --ignore-not-found || echo "Secret cleanup completed"
            kubectl delete configmap --all -n llm --ignore-not-found || echo "ConfigMaps cleanup completed"
            
            echo "â³ Waiting for cleanup to complete (15s)..."
            sleep 15
          else
            echo "ðŸ†• Fresh LLM namespace - no cleanup needed"
          fi
          
          # Handle observability namespace more conservatively
          if kubectl get namespace observability &>/dev/null; then
            echo "ðŸ“Š Observability namespace exists - performing selective cleanup..."
            
            # Only clean up if there are issues, otherwise keep the stack running
            FAILED_PODS=$(kubectl get pods -n observability --field-selector=status.phase=Failed --no-headers 2>/dev/null | wc -l || echo "0")
            if [[ "$FAILED_PODS" -gt 0 ]]; then
              echo "Found $FAILED_PODS failed pods, cleaning up observability..."
              kubectl delete namespace observability --ignore-not-found=true --timeout=60s || echo "Observability cleanup completed"
              sleep 15
            else
              echo "Observability stack healthy - keeping existing deployment"
            fi
          else
            echo "ðŸ†• Fresh observability namespace"
          fi
          
          echo "âœ… Smart cleanup completed"

      - name: Deploy Kong Gateway
        run: |
          # Create namespace for LLM services (Kong will be installed here)
          kubectl create namespace llm --dry-run=client -o yaml | kubectl apply -f -
          
          # Create GitHub Container Registry pull secret
          kubectl create secret docker-registry ghcr-pull \
            --namespace llm \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Install Kong as ingress controller with force upgrade
          echo "ðŸ” Installing Kong Gateway..."
          helm upgrade --install kong kong/kong \
            --namespace llm \
            --set ingressController.enabled=true \
            --set admin.enabled=true \
            --set admin.http.enabled=true \
            --set proxy.type=LoadBalancer \
            --wait \
            --timeout 8m \
            --force \
             || {
              echo "âŒ Kong deployment failed"
              kubectl get pods -n llm
              exit 1
            }
          
          echo "âœ… Kong Gateway deployed successfully"

      - name: Setup Terraform for Kong IP Update
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0

      - name: Update Front Door Origin with Kong IP
        run: |
          # Set up Terraform environment variables
          CLIENT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientId')
          CLIENT_SECRET=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientSecret')
          TENANT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.tenantId')
          SUBSCRIPTION_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.subscriptionId')
          
          export ARM_CLIENT_ID=$CLIENT_ID
          export ARM_CLIENT_SECRET=$CLIENT_SECRET
          export ARM_TENANT_ID=$TENANT_ID
          export ARM_SUBSCRIPTION_ID=$SUBSCRIPTION_ID
          export ARM_USE_CLI=false
          
          # Wait for Kong LoadBalancer to get external IP
          echo "â³ Waiting for Kong LoadBalancer to get external IP..."
          
          # Wait up to 5 minutes for external IP
          for i in {1..30}; do
            KONG_IP=$(kubectl get service kong-kong-proxy -n llm -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            if [[ -n "$KONG_IP" && "$KONG_IP" != "null" && "$KONG_IP" != "" ]]; then
              echo "ðŸ”— Kong LoadBalancer IP: $KONG_IP"
              break
            fi
            echo "   Attempt $i/30: Waiting for Kong IP..."
            sleep 10
          done
          
          if [[ -n "$KONG_IP" && "$KONG_IP" != "null" && "$KONG_IP" != "" ]]; then
            # Get current IP from terraform.tfvars
            CURRENT_IP=$(grep "frontdoor_origin_hostname" infra/envs/azure/dev/terraform.tfvars | cut -d'"' -f2)
            
            if [[ "$CURRENT_IP" != "$KONG_IP" ]]; then
              echo "ðŸ”„ Updating Front Door origin from $CURRENT_IP to $KONG_IP"
              
              # Update terraform.tfvars with new Kong IP
              sed -i "s/frontdoor_origin_hostname.*=.*/frontdoor_origin_hostname    = \"$KONG_IP\"  # Auto-updated by workflow/" infra/envs/azure/dev/terraform.tfvars
              
              # Initialize and apply terraform to update Front Door origin
              cd infra/envs/azure/dev
              terraform init
              terraform apply -auto-approve
              echo "âœ… Front Door origin updated to $KONG_IP"
            else
              echo "âœ… Front Door origin already points to correct IP: $KONG_IP"
            fi
          else
            echo "âš ï¸ Could not get Kong IP after 5 minutes, Front Door origin not updated"
            echo "   You may need to manually update frontdoor_origin_hostname in terraform.tfvars"
          fi

      - name: Deploy Observability Stack
        run: |
          # Create namespace
          kubectl create namespace observability --dry-run=client -o yaml | kubectl apply -f -
          
          # Check cluster resources before deploying observability
          echo "ðŸ“Š Pre-deployment cluster status:"
          kubectl get nodes -o wide
          kubectl describe nodes | grep -A 10 "Allocated resources"
          kubectl get pods --all-namespaces | wc -l
          echo "Current pod count: $(kubectl get pods --all-namespaces --no-headers | wc -l)"
          
          # Configure observability workloads to use spot instances when needed
          echo "ðŸŽ¯ Configuring spot instance tolerations for observability workloads..."
          
          # Deploy Loki for logging (with spot instance tolerance)
          echo "ðŸ” Deploying Loki with spot instance support..."
          helm upgrade --install loki grafana/loki \
            --namespace observability \
            --create-namespace \
            -f observability/parameters/loki.yaml \
            --set tolerations[0].key=kubernetes.azure.com/scalesetpriority \
            --set tolerations[0].operator=Equal \
            --set tolerations[0].value=spot \
            --set tolerations[0].effect=NoSchedule \
            --timeout 5m \
            --debug \
             || {
              echo "âš ï¸ Loki deployment failed, checking status..."
              kubectl get pods -n observability
              kubectl describe pods -n observability | head -100
              echo "Continuing without waiting for Loki..."
            }
          
          # Deploy Promtail RBAC and DaemonSet for log collection
          echo "ðŸ” Deploying Promtail for log collection..."
          kubectl apply -f observability/promtail-rbac.yaml || echo "âš ï¸ Promtail RBAC apply failed"
          kubectl apply -f observability/parameters/promtail.yaml || echo "âš ï¸ Promtail DaemonSet apply failed"
          
          # Deploy Prometheus for metrics (with debugging)
          echo "ðŸ” Deploying Prometheus..."
          helm upgrade --install prometheus prometheus-community/prometheus \
            --namespace observability \
            --create-namespace \
            --timeout 3m \
            --debug \
             || {
              echo "âš ï¸ Prometheus deployment failed, checking status..."
              kubectl get pods -n observability
              echo "Continuing without waiting for Prometheus..."
            }
          
          # Deploy Grafana for dashboards (with spot instance support)
          echo "ðŸ” Deploying Grafana with spot instance tolerance..."
          helm upgrade --install grafana grafana/grafana \
            --namespace observability \
            --create-namespace \
            --set adminPassword=${{ secrets.GRAFANA_ADMIN_PASSWORD }} \
            --set sidecar.dashboards.enabled=true \
            --set sidecar.dashboards.label=grafana_dashboard \
            --set sidecar.datasources.enabled=true \
            --set sidecar.datasources.label=grafana_datasource \
            --set tolerations[0].key=kubernetes.azure.com/scalesetpriority \
            --set tolerations[0].operator=Equal \
            --set tolerations[0].value=spot \
            --set tolerations[0].effect=NoSchedule \
            --set resources.requests.cpu=100m \
            --set resources.requests.memory=128Mi \
            --set resources.limits.cpu=500m \
            --set resources.limits.memory=512Mi \
            --timeout 5m \
            --debug \
             || {
              echo "âš ï¸ Grafana deployment failed, checking status..."
              kubectl get pods -n observability
              echo "Continuing without waiting for Grafana..."
            }
          
          # Apply comprehensive Grafana configuration (consolidated)
          echo "ðŸš€ Applying comprehensive Grafana configuration..."
          kubectl apply -f observability/parameters/grafana.yaml || echo "âš ï¸ Grafana configuration failed"
          
          # Restart Grafana to pick up new configuration
          echo "ðŸ”„ Restarting Grafana to apply configuration..."
          kubectl rollout restart deployment/grafana -n observability || echo "âš ï¸ Grafana restart failed"
          
          # Apply custom dashboards and configs (best effort)
          echo "ðŸ” Applying custom dashboards and configurations..."
          kubectl apply -f observability/grafana-llm-analytics-dashboard.yaml || echo "âš ï¸ LLM analytics dashboard apply failed"
          kubectl apply -f observability/grafana-mlops-dashboard.yaml || echo "âš ï¸ MLOps dashboard apply failed"
          kubectl apply -f observability/loki-datasource.yaml || echo "âš ï¸ Loki datasource apply failed"

      - name: Deploy MLOps Services
        run: |
          # Ensure namespace exists
          kubectl create namespace llm --dry-run=client -o yaml | kubectl apply -f -
          
          # Create or update GHCR pull secret
          kubectl create secret docker-registry ghcr-pull \
            --namespace llm \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Deploy Guardrail service
          echo "ðŸ” Deploying Guardrail service..."
          helm upgrade --install guardrail charts/guardrail \
               --namespace llm \
               --set image.repository=ghcr.io/${{ github.repository_owner }}/guardrail \
               --set image.tag=${{ github.sha }} \
               --set image.pullSecrets[0].name=ghcr-pull \
               --set replicaCount=1 \
               --timeout 5m \
               --wait \
               --force \
                || {
            echo "âš ï¸ Guardrail deployment failed, checking status..."
            kubectl get pods -n llm
            kubectl describe pods -n llm -l app.kubernetes.io/name=guardrail | tail -20
          }
          
          # Deploy LLM Proxy service
          echo "ðŸ” Deploying LLM Proxy service..."
          helm upgrade --install llm-proxy charts/llm-proxy \
               --namespace llm \
               --set image.repository=ghcr.io/${{ github.repository_owner }}/llm-proxy \
               --set image.tag=${{ github.sha }} \
               --set image.pullSecrets[0].name=ghcr-pull \
               --set replicaCount=1 \
               --set env.AZURE_OPENAI_ENDPOINT="${{ needs.infrastructure.outputs.azure_openai_endpoint }}" \
               --set env.AZURE_OPENAI_API_KEY="${{ needs.infrastructure.outputs.azure_openai_api_key }}" \
               --set env.AZURE_OPENAI_DEPLOYMENT_NAME="${{ needs.infrastructure.outputs.gpt4o_deployment_name }}" \
               --set env.AZURE_OPENAI_API_VERSION="2024-08-01-preview" \
               --timeout 5m \
               --wait \
               --force \
                || {
            echo "âš ï¸ LLM Proxy deployment failed, checking status..."
            kubectl get pods -n llm
            kubectl describe pods -n llm -l app.kubernetes.io/name=llm-proxy | tail -20
          }
          
          # Show current pod status for debugging
          echo "ðŸ“Š Current pod status in llm namespace:"
          kubectl get pods -n llm -o wide
          
          # Verify services are ready
          echo "â³ Verifying services are ready..."
          kubectl get deployment guardrail -n llm || echo "âš ï¸ Guardrail deployment not found"
          kubectl get deployment llm-proxy -n llm || echo "âš ï¸ LLM-proxy deployment not found"

      - name: Deploy Gateway and Security
        run: |
          # First, clean up any existing ingresses that might conflict
          echo "ðŸ§¹ Cleaning up existing Grafana ingress in observability namespace..."
          kubectl delete ingress grafana-ingress -n observability --ignore-not-found=true
          
          # Deploy Kong gateway configuration (ingress) using Helm
          echo "ðŸ” Deploying Kong gateway configurations..."
          if helm upgrade --install gateway charts/gateway/ \
              --namespace llm \
              --set ingress.host="agent.reisdematos.ch" \
              --set ingress.tls.secretName="llm-tls" \
              --force; then
            echo "âœ… Gateway deployed successfully"
          else
            echo "âš ï¸ Gateway deployment failed, trying fallback..."
            # Fallback: try without custom domain (use default from values)
            helm upgrade --install gateway charts/gateway/ \
              --namespace llm \
              --set ingress.tls.secretName="llm-tls" \
              --force || echo "âŒ Gateway deployment failed completely"
          fi
          
          # Apply Kong observability configurations (now that Kong CRDs are available) - best effort
          echo "ðŸ” Applying Kong observability configurations..."
          kubectl apply -f observability/kong-metrics.yaml || echo "âš ï¸ Kong metrics config failed"
          kubectl apply -f observability/kong-servicemonitor.yaml || echo "âš ï¸ Kong service monitor failed"
          
          # Apply security policies with detailed feedback
          echo "ðŸ” Applying security policies..."
          
          # Apply each security file individually for better error tracking
          for security_file in security/*.yaml; do
            if [[ -f "$security_file" ]]; then
              echo "ðŸ“‹ Applying $security_file..."
              if kubectl apply -f "$security_file"; then
                echo "âœ… Successfully applied $security_file"
              else
                echo "âŒ Failed to apply $security_file"
              fi
            fi
          done
          
          # Verify security policies are active
          echo "ðŸ”’ Verifying security policies are active..."
          kubectl get networkpolicies -A || echo "No network policies found"
          kubectl get kongplugins -A || echo "No Kong plugins found"

      - name: Verify Deployment
        run: |
          # Show final deployment status
          echo "ðŸ” Checking final deployment status..."
          kubectl get deployments -n llm -o wide
          kubectl get deployments -n observability -o wide
          
          # Show node resource usage to verify we have capacity
          echo "ðŸ“Š Node resource usage:"
          kubectl describe nodes | grep -A 10 "Allocated resources"
          
          # Show all pods status after fresh deployment
          echo "ðŸš€ All pods status after fresh deployment:"
          kubectl get pods --all-namespaces -o wide
          
          # Verify services are healthy
          echo "ðŸ¥ Service health checks:"
          kubectl get pods -n llm -l app.kubernetes.io/name=guardrail
          kubectl get pods -n llm -l app.kubernetes.io/name=llm-proxy
          kubectl get pods -n llm -l app.kubernetes.io/name=kong
          
          # Get service endpoints
          echo "ðŸŒ Service endpoints:"
          kubectl get services -A
          kubectl get ingress -A
          
          # Show logs for any failing pods
          echo "ðŸ“‹ Checking for any failing pods..."
          kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded

      - name: Final Deployment Status
        run: |
          echo "ðŸŽ‰ ========================================"
          echo "ðŸŽ‰ MLOps Platform Deployment Complete!"
          echo "ðŸŽ‰ ========================================"
          
          # Get Kong LoadBalancer IP
          KONG_IP=$(kubectl get service kong-kong-proxy -n llm -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          
          echo ""
          echo "ðŸŒ Access URLs:"
          echo "   Custom Domain: https://agent.reisdematos.ch"
          echo "   Front Door:    https://${{ needs.infrastructure.outputs.frontdoor_endpoint_hostname }}"
          echo "   Kong Direct:   http://$KONG_IP"
          echo ""
          echo "ðŸ”— Available Endpoints:"
          echo "   Health Check:  https://agent.reisdematos.ch/healthz"
          echo "   Sanitize:      https://agent.reisdematos.ch/sanitize"
          echo "   Chat:          https://agent.reisdematos.ch/chat"
          echo "   Grafana:       https://agent.reisdematos.ch/grafana"
          echo ""
          echo "ðŸ“Š Resource Status:"
          kubectl get pods -n llm --no-headers | wc -l | xargs echo "   LLM Namespace Pods:"
          kubectl get pods -n observability --no-headers | wc -l | xargs echo "   Observability Pods:"
          echo ""
          echo "âœ… Deployment verification complete!"

      - name: Display Cost Optimization Status
        run: |
          echo "ðŸ’° ========================================"
          echo "ðŸ’° Cost Optimization Status"
          echo "ðŸ’° ========================================"
          
          # Show node status and cost information
          echo ""
          echo "ðŸ–¥ï¸ Node Pool Status:"
          kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,INSTANCE-TYPE:.metadata.labels.beta\\.kubernetes\\.io/instance-type,ZONE:.metadata.labels.topology\\.kubernetes\\.io/zone
          
          # Show spot vs regular node allocation
          echo ""
          echo "ðŸ“Š Pod Distribution:"
          echo "Regular nodes:"
          kubectl get pods --all-namespaces -o wide | grep -v spot | wc -l | xargs echo "  Pods on regular nodes:"
          
          SPOT_NODES=$(kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot --no-headers 2>/dev/null | wc -l || echo "0")
          echo "  Spot nodes available: $SPOT_NODES"
          
          if [[ "$SPOT_NODES" -gt 0 ]]; then
            kubectl get pods --all-namespaces -o wide | grep spot | wc -l | xargs echo "  Pods on spot nodes:"
          else
            echo "  Pods on spot nodes: 0 (spot nodes will scale up when needed)"
          fi
          
          echo ""
          echo "ðŸ’¡ Cost Savings Enabled:"
          echo "  âœ… Spot instances: Up to 90% savings on burst capacity"
          echo "  âœ… Auto-scaling: Pay only for what you use (min 0, max 2 spot nodes)"
          echo "  âœ… Ephemeral disks: Faster and cheaper storage"
          echo ""
          echo "ðŸ“ˆ Expected Monthly Costs:"
          echo "  â€¢ Base infrastructure: ~$60-80/month (regular node)"
          echo "  â€¢ Burst capacity: $0-24/month (0-2 spot nodes as needed)"
          echo "  â€¢ Average total: ~$70-90/month (vs $120-180 without optimization)"
          echo ""
          echo "ðŸŽ¯ Grafana will automatically deploy to spot instances when available!"
