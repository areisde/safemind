name: Deploy MLOps Platform
on: 
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AZURE_RESOURCE_GROUP: k8s-dev-aks-rg
  AKS_CLUSTER_NAME: k8s-dev
  IMAGE_GUARDRAIL: ghcr.io/${{ github.repository_owner }}/guardrail:${{ github.sha }}
  IMAGE_PROXY: ghcr.io/${{ github.repository_owner }}/llm-proxy:${{ github.sha }}

jobs:
  infrastructure:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    outputs:
      kubeconfig: ${{ steps.get-kubeconfig.outputs.kubeconfig }}
      azure_openai_endpoint: ${{ steps.terraform-outputs.outputs.azure_openai_endpoint }}
      azure_openai_api_key: ${{ steps.terraform-outputs.outputs.azure_openai_api_key }}
      gpt4o_deployment_name: ${{ steps.terraform-outputs.outputs.gpt4o_deployment_name }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0
      
      - name: Set Terraform Azure Provider Environment Variables
        run: |
          # Extract values from AZURE_CREDENTIALS secret
          CLIENT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientId')
          CLIENT_SECRET=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientSecret')
          TENANT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.tenantId')
          SUBSCRIPTION_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.subscriptionId')
          
          # Set environment variables for Terraform Azure Provider
          echo "ARM_CLIENT_ID=$CLIENT_ID" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=$CLIENT_SECRET" >> $GITHUB_ENV
          echo "ARM_TENANT_ID=$TENANT_ID" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=$SUBSCRIPTION_ID" >> $GITHUB_ENV
          echo "ARM_USE_CLI=false" >> $GITHUB_ENV
      
      - name: Terraform Init
        working-directory: infra/envs/azure/dev
        run: terraform init
      
      - name: Terraform Plan
        working-directory: infra/envs/azure/dev
        run: terraform plan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        working-directory: infra/envs/azure/dev
        run: terraform apply -auto-approve

      - name: Get AKS credentials
        id: get-kubeconfig
        run: |
          az aks get-credentials --resource-group ${{ env.AZURE_RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }} --file kubeconfig
          echo "kubeconfig=$(cat kubeconfig | base64 -w 0)" >> $GITHUB_OUTPUT

      - name: Get Terraform outputs
        id: terraform-outputs
        working-directory: infra/envs/azure/dev
        run: |
          echo "azure_openai_endpoint=$(terraform output -raw azure_openai_endpoint)" >> $GITHUB_OUTPUT
          echo "azure_openai_api_key=$(terraform output -raw azure_openai_api_key)" >> $GITHUB_OUTPUT
          echo "gpt4o_deployment_name=$(terraform output -raw gpt4o_deployment_name)" >> $GITHUB_OUTPUT

  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - name: Log in to GitHub Container Registry
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build & push Guardrail
        run: |
          docker build -t $IMAGE_GUARDRAIL services/guardrail
          docker push $IMAGE_GUARDRAIL

      - name: Build & push LLM-proxy
        run: |
          docker build -t $IMAGE_PROXY services/llm-proxy
          docker push $IMAGE_PROXY

  deploy:
    needs: [infrastructure, build]
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Set Azure Environment Variables
        run: |
          # Extract values from AZURE_CREDENTIALS secret for deploy job
          CLIENT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientId')
          CLIENT_SECRET=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.clientSecret')
          TENANT_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.tenantId')
          SUBSCRIPTION_ID=$(echo '${{ secrets.AZURE_CREDENTIALS }}' | jq -r '.subscriptionId')
          
          # Set environment variables for Azure CLI and subsequent steps
          echo "ARM_CLIENT_ID=$CLIENT_ID" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=$CLIENT_SECRET" >> $GITHUB_ENV
          echo "ARM_TENANT_ID=$TENANT_ID" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=$SUBSCRIPTION_ID" >> $GITHUB_ENV

      - name: Setup kubectl
        run: |
          echo "${{ needs.infrastructure.outputs.kubeconfig }}" | base64 -d > kubeconfig
          chmod 600 kubeconfig

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Add Helm repositories
        run: |
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add kong https://charts.konghq.com
          helm repo update

      - name: Cleanup Existing Deployments
        run: |
          echo "üßπ Cleaning up existing deployments for fresh start..."
          
          # More comprehensive Helm cleanup with proper resource deletion
          echo "Removing existing Helm releases with full cleanup..."
          
          # Uninstall Helm releases and wait for complete removal
          helm --kubeconfig=kubeconfig uninstall kong -n llm --wait || echo "Kong release not found"
          helm --kubeconfig=kubeconfig uninstall guardrail -n llm --wait || echo "Guardrail release not found"
          helm --kubeconfig=kubeconfig uninstall llm-proxy -n llm --wait || echo "LLM Proxy release not found"
          helm --kubeconfig=kubeconfig uninstall loki -n observability --wait || echo "Loki release not found"
          helm --kubeconfig=kubeconfig uninstall prometheus -n observability --wait || echo "Prometheus release not found"
          helm --kubeconfig=kubeconfig uninstall grafana -n observability --wait || echo "Grafana release not found"
          
          # Wait for Helm releases to be fully removed
          echo "‚è≥ Waiting for Helm releases to be completely removed..."
          sleep 30
          
          # Force delete remaining resources by type (including those missed by Helm)
          echo "Force cleaning all remaining resources..."
          for namespace in llm observability; do
            echo "Cleaning namespace: $namespace"
            
            # Delete all workload resources
            kubectl --kubeconfig=kubeconfig delete deployments,statefulsets,daemonsets,replicasets,jobs,cronjobs --all -n $namespace --ignore-not-found=true --force --grace-period=0 || echo "Workloads cleared in $namespace"
            
            # Delete all networking resources
            kubectl --kubeconfig=kubeconfig delete services,ingresses,networkpolicies --all -n $namespace --ignore-not-found=true --force --grace-period=0 || echo "Networking cleared in $namespace"
            
            # Delete all configuration and storage resources
            kubectl --kubeconfig=kubeconfig delete configmaps,secrets,persistentvolumeclaims --all -n $namespace --ignore-not-found=true --force --grace-period=0 || echo "Config/storage cleared in $namespace"
            
            # Delete all RBAC resources
            kubectl --kubeconfig=kubeconfig delete serviceaccounts,roles,rolebindings,clusterroles,clusterrolebindings --all -n $namespace --ignore-not-found=true --force --grace-period=0 || echo "RBAC cleared in $namespace"
            
            # Force delete any remaining pods
            kubectl --kubeconfig=kubeconfig delete pods --all -n $namespace --ignore-not-found=true --force --grace-period=0 || echo "Pods force-deleted in $namespace"
          done
          
          # Delete the namespaces completely
          echo "Deleting namespaces completely..."
          kubectl --kubeconfig=kubeconfig delete namespace llm --ignore-not-found=true --force --grace-period=0 || echo "LLM namespace force-deleted"
          kubectl --kubeconfig=kubeconfig delete namespace observability --ignore-not-found=true --force --grace-period=0 || echo "Observability namespace force-deleted"
          
          # Wait for complete cleanup
          echo "‚è≥ Waiting for complete namespace cleanup..."
          sleep 45
          
          # Show current state
          echo "üìä Current cluster state after cleanup:"
          kubectl --kubeconfig=kubeconfig get pods --all-namespaces | grep -E "(llm|observability)" || echo "‚úÖ No MLOps pods remaining"
          kubectl --kubeconfig=kubeconfig get namespaces

      - name: Deploy Kong Gateway
        run: |
          # Create namespace for LLM services (Kong will be installed here)
          kubectl --kubeconfig=kubeconfig create namespace llm --dry-run=client -o yaml | kubectl --kubeconfig=kubeconfig apply -f -
          
          # Create GitHub Container Registry pull secret
          kubectl --kubeconfig=kubeconfig create secret docker-registry ghcr-pull \
            --namespace llm \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com \
            --dry-run=client -o yaml | kubectl --kubeconfig=kubeconfig apply -f -
          
          # Install Kong as ingress controller with retry logic
          echo "üîç Installing Kong Gateway..."
          if ! helm upgrade --install kong kong/kong \
            --namespace llm \
            --set ingressController.enabled=true \
            --set admin.enabled=true \
            --set admin.http.enabled=true \
            --set proxy.type=LoadBalancer \
            --wait \
            --timeout 8m \
            --kubeconfig kubeconfig; then
            
            echo "‚ö†Ô∏è Kong deployment failed, attempting retry..."
            # Clean up failed Kong deployment
            helm --kubeconfig=kubeconfig uninstall kong -n llm || echo "No kong release to remove"
            kubectl --kubeconfig=kubeconfig delete all -l app.kubernetes.io/name=kong -n llm --ignore-not-found=true || echo "Kong resources cleaned"
            sleep 15
            
            # Retry Kong installation
            helm install kong kong/kong \
              --namespace llm \
              --set ingressController.enabled=true \
              --set admin.enabled=true \
              --set admin.http.enabled=true \
              --set proxy.type=LoadBalancer \
              --wait \
              --timeout 8m \
              --kubeconfig kubeconfig || {
                echo "‚ùå Kong deployment failed on retry"
                kubectl --kubeconfig=kubeconfig get pods -n llm
                exit 1
              }
          fi
          
          echo "‚úÖ Kong Gateway deployed successfully"

      - name: Deploy Observability Stack
        run: |
          # Create namespace
          kubectl --kubeconfig=kubeconfig create namespace observability --dry-run=client -o yaml | kubectl --kubeconfig=kubeconfig apply -f -
          
          # Check cluster resources before deploying observability
          echo "üìä Pre-deployment cluster status:"
          kubectl --kubeconfig=kubeconfig get nodes -o wide
          kubectl --kubeconfig=kubeconfig describe nodes | grep -A 10 "Allocated resources"
          kubectl --kubeconfig=kubeconfig get pods --all-namespaces | wc -l
          echo "Current pod count: $(kubectl --kubeconfig=kubeconfig get pods --all-namespaces --no-headers | wc -l)"
          
          # Deploy Loki for logging (with debugging and fallback)
          echo "üîç Deploying Loki with debugging..."
          echo "Loki configuration:"
          cat observability/parameters/loki.yaml
          
          # Try to deploy Loki with shorter timeout and debugging
          helm upgrade --install loki grafana/loki \
            --namespace observability \
            --create-namespace \
            -f observability/parameters/loki.yaml \
            --timeout 3m \
            --debug \
            --kubeconfig kubeconfig || {
              echo "‚ö†Ô∏è Loki deployment failed, checking status..."
              kubectl --kubeconfig=kubeconfig get pods -n observability
              kubectl --kubeconfig=kubeconfig describe pods -n observability | head -100
              echo "Continuing without waiting for Loki..."
            }
          
          # Deploy Promtail RBAC and DaemonSet for log collection
          echo "üîç Deploying Promtail for log collection..."
          kubectl --kubeconfig=kubeconfig apply -f observability/promtail-rbac.yaml || echo "‚ö†Ô∏è Promtail RBAC apply failed"
          kubectl --kubeconfig=kubeconfig apply -f observability/parameters/promtail.yaml || echo "‚ö†Ô∏è Promtail DaemonSet apply failed"
          
          # Deploy Prometheus for metrics (with debugging)
          echo "üîç Deploying Prometheus..."
          helm upgrade --install prometheus prometheus-community/prometheus \
            --namespace observability \
            --create-namespace \
            --timeout 3m \
            --debug \
            --kubeconfig kubeconfig || {
              echo "‚ö†Ô∏è Prometheus deployment failed, checking status..."
              kubectl --kubeconfig=kubeconfig get pods -n observability
              echo "Continuing without waiting for Prometheus..."
            }
          
          # Deploy Grafana for dashboards (with debugging)
          echo "üîç Deploying Grafana..."
          helm upgrade --install grafana grafana/grafana \
            --namespace observability \
            --create-namespace \
            --set adminPassword=${{ secrets.GRAFANA_ADMIN_PASSWORD }} \
            --set sidecar.dashboards.enabled=true \
            --set sidecar.dashboards.label=grafana_dashboard \
            --set sidecar.datasources.enabled=true \
            --set sidecar.datasources.label=grafana_datasource \
            --timeout 3m \
            --debug \
            --kubeconfig kubeconfig || {
              echo "‚ö†Ô∏è Grafana deployment failed, checking status..."
              kubectl --kubeconfig=kubeconfig get pods -n observability
              echo "Continuing without waiting for Grafana..."
            }
          
          # Apply custom dashboards and configs (best effort)
          echo "üîç Applying custom dashboards and configurations..."
          kubectl --kubeconfig=kubeconfig apply -f observability/grafana-llm-analytics-dashboard.yaml || echo "‚ö†Ô∏è LLM analytics dashboard apply failed"
          kubectl --kubeconfig=kubeconfig apply -f observability/grafana-mlops-dashboard.yaml || echo "‚ö†Ô∏è MLOps dashboard apply failed"
          kubectl --kubeconfig=kubeconfig apply -f observability/loki-datasource.yaml || echo "‚ö†Ô∏è Loki datasource apply failed"

      - name: Deploy MLOps Services
        run: |
          # Ensure namespace exists and is clean
          kubectl --kubeconfig=kubeconfig create namespace llm --dry-run=client -o yaml | kubectl --kubeconfig=kubeconfig apply -f -
          
          # Recreate GHCR pull secret in case it was cleaned up
          kubectl --kubeconfig=kubeconfig create secret docker-registry ghcr-pull \
            --namespace llm \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com \
            --dry-run=client -o yaml | kubectl --kubeconfig=kubeconfig apply -f -
          
          # Deploy Guardrail service with conflict resolution
          echo "üîç Deploying Guardrail service..."
          # First attempt: try normal upgrade/install
          if ! helm upgrade --install guardrail charts/guardrail \
               --namespace llm \
               --set image.repository=ghcr.io/${{ github.repository_owner }}/guardrail \
               --set image.tag=${{ github.sha }} \
               --timeout 5m \
               --kubeconfig kubeconfig; then
            
            echo "‚ö†Ô∏è Guardrail deployment failed, attempting cleanup and retry..."
            # Cleanup any partial deployment
            helm --kubeconfig=kubeconfig uninstall guardrail -n llm || echo "No guardrail release to remove"
            kubectl --kubeconfig=kubeconfig delete all -l app.kubernetes.io/name=guardrail -n llm --ignore-not-found=true || echo "Guardrail resources cleaned"
            sleep 10
            
            # Second attempt: fresh install
            helm install guardrail charts/guardrail \
                 --namespace llm \
                 --set image.repository=ghcr.io/${{ github.repository_owner }}/guardrail \
                 --set image.tag=${{ github.sha }} \
                 --timeout 5m \
                 --kubeconfig kubeconfig || {
              echo "‚ùå Guardrail deployment failed on retry"
              kubectl --kubeconfig=kubeconfig get pods -n llm
              kubectl --kubeconfig=kubeconfig describe pods -n llm | grep -A 10 "guardrail"
            }
          fi
          
          # Deploy LLM Proxy service with conflict resolution
          echo "üîç Deploying LLM Proxy service..."
          # First attempt: try normal upgrade/install
          if ! helm upgrade --install llm-proxy charts/llm-proxy \
               --namespace llm \
               --set image.repository=ghcr.io/${{ github.repository_owner }}/llm-proxy \
               --set image.tag=${{ github.sha }} \
               --set env.AZURE_OPENAI_ENDPOINT="${{ needs.infrastructure.outputs.azure_openai_endpoint }}" \
               --set env.AZURE_OPENAI_API_KEY="${{ needs.infrastructure.outputs.azure_openai_api_key }}" \
               --set env.AZURE_OPENAI_DEPLOYMENT_NAME="${{ needs.infrastructure.outputs.gpt4o_deployment_name }}" \
               --set env.AZURE_OPENAI_API_VERSION="2024-08-01-preview" \
               --timeout 5m \
               --kubeconfig kubeconfig; then
            
            echo "‚ö†Ô∏è LLM Proxy deployment failed, attempting cleanup and retry..."
            # Cleanup any partial deployment
            helm --kubeconfig=kubeconfig uninstall llm-proxy -n llm || echo "No llm-proxy release to remove"
            kubectl --kubeconfig=kubeconfig delete all -l app.kubernetes.io/name=llm-proxy -n llm --ignore-not-found=true || echo "LLM Proxy resources cleaned"
            sleep 10
            
            # Second attempt: fresh install
            helm install llm-proxy charts/llm-proxy \
                 --namespace llm \
                 --set image.repository=ghcr.io/${{ github.repository_owner }}/llm-proxy \
                 --set image.tag=${{ github.sha }} \
                 --set env.AZURE_OPENAI_ENDPOINT="${{ needs.infrastructure.outputs.azure_openai_endpoint }}" \
                 --set env.AZURE_OPENAI_API_KEY="${{ needs.infrastructure.outputs.azure_openai_api_key }}" \
                 --set env.AZURE_OPENAI_DEPLOYMENT_NAME="${{ needs.infrastructure.outputs.gpt4o_deployment_name }}" \
                 --set env.AZURE_OPENAI_API_VERSION="2024-08-01-preview" \
                 --timeout 5m \
                 --kubeconfig kubeconfig || {
              echo "‚ùå LLM Proxy deployment failed on retry"
              kubectl --kubeconfig=kubeconfig get pods -n llm
              kubectl --kubeconfig=kubeconfig describe pods -n llm | grep -A 10 "llm-proxy"
            }
          fi
          
          # Show current pod status for debugging
          echo "üìä Current pod status in llm namespace:"
          kubectl --kubeconfig=kubeconfig get pods -n llm -o wide
          
          # Wait for critical services with shorter timeout (best effort)
          echo "‚è≥ Waiting for critical services (best effort)..."
          kubectl --kubeconfig=kubeconfig wait --for=condition=available deployment/guardrail -n llm --timeout=180s || echo "‚ö†Ô∏è Guardrail still starting..."
          kubectl --kubeconfig=kubeconfig wait --for=condition=available deployment/llm-proxy -n llm --timeout=180s || echo "‚ö†Ô∏è LLM Proxy still starting..."

      - name: Deploy Gateway and Security
        run: |
          # Apply Kong gateway configuration (ingress) - best effort
          echo "üîç Applying Kong gateway configurations..."
          kubectl --kubeconfig=kubeconfig apply -f charts/gateway/ -R || echo "‚ö†Ô∏è Some gateway configs failed to apply"
          
          # Apply Kong observability configurations (now that Kong CRDs are available) - best effort
          echo "üîç Applying Kong observability configurations..."
          kubectl --kubeconfig=kubeconfig apply -f observability/kong-metrics.yaml || echo "‚ö†Ô∏è Kong metrics config failed"
          kubectl --kubeconfig=kubeconfig apply -f observability/kong-servicemonitor.yaml || echo "‚ö†Ô∏è Kong service monitor failed"
          
          # Apply security policies - best effort
          echo "üîç Applying security policies..."
          kubectl --kubeconfig=kubeconfig apply -f security/ -R || echo "‚ö†Ô∏è Some security policies failed to apply"

      - name: Verify Deployment
        run: |
          # Show final deployment status
          echo "üîç Checking final deployment status..."
          kubectl --kubeconfig=kubeconfig get deployments -n llm -o wide
          kubectl --kubeconfig=kubeconfig get deployments -n observability -o wide
          
          # Show node resource usage to verify we have capacity
          echo "üìä Node resource usage:"
          kubectl --kubeconfig=kubeconfig describe nodes | grep -A 10 "Allocated resources"
          
          # Show all pods status after fresh deployment
          echo "üöÄ All pods status after fresh deployment:"
          kubectl --kubeconfig=kubeconfig get pods --all-namespaces -o wide
          
          # Verify services are healthy
          echo "üè• Service health checks:"
          kubectl --kubeconfig=kubeconfig get pods -n llm -l app.kubernetes.io/name=guardrail
          kubectl --kubeconfig=kubeconfig get pods -n llm -l app.kubernetes.io/name=llm-proxy
          kubectl --kubeconfig=kubeconfig get pods -n llm -l app.kubernetes.io/name=kong
          
          # Get service endpoints
          echo "üåê Service endpoints:"
          kubectl --kubeconfig=kubeconfig get services -A
          kubectl --kubeconfig=kubeconfig get ingress -A
          
          # Show logs for any failing pods
          echo "üìã Checking for any failing pods..."
          kubectl --kubeconfig=kubeconfig get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded
